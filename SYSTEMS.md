You can think of this project as having two big parts.

1. materialist, hosted at materialist.pages.dev, which includes:

   - `baro-data.py`, a Python script to build recipe list data from barotrauma vanilla & mod content files.
   - `web/`, TypeScript & SolidJS website sources for viewing recipe lists, can be build as static files.

2. splicer, hosted at materialist-splicer.pages.dev, which includes:

   - `splicer/`, Rust & Python services for automatically building and deploying new recipe list.
   - `splicer-web/`, TypeScript & SolidJS website sources to allow users to submit new recipe list to the splicer services.

More details follow:

baro-data.py
------------

`baro-data.py` is a script for extracting crafting recipes using content from Barotrauma and Barotrauma mods.

Given provided paths, it reads .xml and image files to generate a .json and .css file for each load order. Multiple load orders can be specified at a time. The load orders are then listed under a generated TypeScript file, `index.ts`, that is used by `web` as a source for recipe data.

The script is used by `splicer` through a container image. The Containerfile is at `splicer/build/Containerfile`.

web
---

`web` is a website that uses JavaScript in your web browser to view recipes generated by `baro-data.py`.

It looks for recipe data (as .json and .css files, and one `index.ts`) in `web/assets/bundles` and serves the recipe lists from that index.

It can be built as a static website with `vite build` which bundles the recipe data. This is used by `splicer` build and upload a new versions of `materialist.pages.dev` automatically after a new mod load order is built on `materialist-splicer.pages.dev`. The Containerfile is at `splicer/publish/Containerfile`.

splicer
-------

There are two services, `splicer/api` and `splicer/steamcmd`.

- `api/` the main service for handling requests sent from `splicer-web`
- `proxy/` a Cloudflare worker that acts as a proxy between `splicer-web`
  deployed on Cloudflare pages and `api` hosted on my infrastructure
- `build/` a Containerfile for the `splicer-build` image to build recipe lists through automation
- `publish/` a script and Containerfile for the `splicer-publish` image for deploying `splicer-web` through automation
- `steamcmd/` (aka `splicer-steamcmd`) is a service that uses steamcmd to download workshop files
- `autodaemon/` a proc-macro rust library for `api`
- `autodaemon-test/` unit testing for a `autodaemon`
- `stupid-rate-limit/` a library for `api`

Both `splicer/api` and `splicer/steamcmd` services talk to podman to start containers. Here is a diagram where `A ━━● B` means `A` sends to `B`.


```
┌───────────────┐        ┌────────────────────┐
│  splicer-web  │    ┏━━━● steamcommunity.com │
└───────────────┘    ┃┏━━● cloudflare pages   │
           ┃         ┃┃┏━● steam CDNs         │
┌──────────●────┐    ┃┃┃ └────────────────────┘
│ splicer/proxy │    ┃┃┃ 
└───────────────┘    ┃┃┃ 
           ┃         ┃┃┃                     internet
-----------┃---------┃┃┃-----------------------------
           ┃         ┃┃┃                         host
┌──────────●───────┐ ┃┃┃ ┌────────────────────┐
│   splicer/api    │━┛┃┗━│ docker.io/steamcmd ●━━━┓ 
└──────────────────┘  ┗━━│    splicer-publish ●━┓ ┃
           ┃ ┃ ┗━━━━━━━━━●      splicer-build │ ┃ ┃
           ┃ ┃           │       [via podman] │ ┃ ┃
           ┃ ┃           └────────────────────┘ ┃ ┃
           ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┌──────────●───────┐                              ┃
│ splicer/steamcmd │━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
└──────────────────┘
```

- `api` accepts requests from `splicer-web`, the user interface for humans to submit load orders.
- `api` sends HTTP requests to the web pages at steamcommunity.com for workshop files to find information about them, including the workshop item title, authors, latest version, and collection items.
- `api` sends requests to the `splicer/steamcmd` service to download workshop files from steam.
- `api` sends requests to podman over a unix socket to run a `splicer-build` container to run `baro-build.py` and generate recipe lists for a user-submitted load order.
- `api` also uses podman to run a `splicer-publish` container and publish the new recipe lists whenever a user-submitted load order is built.
- `api` requires a volume containing cloudflare secrets in order to publish new recipe lists.
- `api` requires a volume containing Barotrauma's vanilla content files, including image/sprite assets.
- `api` uses a sqlite3 database to store most things, including downloaded workshop files and the .json and .css files from `baro-build.py`.

- `splicer/steamcmd` does not keep state between runs, but it does require access to podman in order to run the Valve's steamcmd program using the container image on docker called steamcmd.

Both `api` and `splicer/steamcmd` need to share a volume with some of the containers they start.

systemd/podman/networking details
---------------------------------

These are notes on how I set this up on my machine. I'm using systemd user services, socket activation, and rootless podman. This isn't the only way to do it. These are just my notes.

Below is a silly diagram of the systemd user units.

```
                                host | network namespace
                                     |
     ┌────────────────┐              |     ┌─────────────────┐
     │ spl-api.socket │━━»activates»━━━━━━━● spl-api.service │━━━━━━┓
     └────────────────┘              |     └─────────────────┘      ┃
                                     |                              ┃
┌─────────────────────┐              |     ┌──────────────────────┐ ┃
│ spl-steamcmd.socket │━━»activates»━━━━━━━● spl-steamcmd.service │━┫
└─────────────────────┘              |     └──────────────────────┘ ┃
                                     |                              ┃
                    ┏━━━━«requires«━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                    ┃                | 
    ┌───────────────●─┐              |     ┌─────────────────────┐
    │ spl-net.service │━━»requires»━━━━━┳━━● spl-dnsmasq.service │
    └─────────────────┘              |  ┃  └─────────────────────┘
                  ┃ ┃                |  ┃
                  ┃ ┃     creates    |  ┃  ┌────────────────────┐
                  ┃ ┗━━━━»network»━━━●  ┗━━● spl-stubby.service │
                  ┃      namespace   |     └────────────────────┘
                  ┃                  |
                  ┃                  |     ┌───────────────┐
                  ┗━━━━━━━»runs»━━━━━|━━━━━● setup-spl-net │
                                     |     └───────────────┘
                                     |       ┃
                    ┏━━━«connects«━━━|━━━━━━━┛
                    ┃                |
┌───────────────────●─┐              |     ┌─────────────────────┐
│ link-friend.service │━━»creates»━┳━|━━━━━● spl-host 10.23.23.2 │
└─────────────────────┘            ┃ |     └─────────────────────┘
                                   ┃ |
┌─────────────────────┐            ┃ |
│ spl-peer 10.23.23.1 ●━━━━━━━━━━━━┛ |
└─────────────────────┘              |
                                     |
```

The services run under the same pod and share a network namespace. The pod, `spl-net`, is created with `--network=none`, which creates a namespace without any networking set up. Instead of using podman's networking, a script called `link-friend.py` creates a veth pair with one end in the pod's network namespace. That link is used by outgoing connections for the pod including DNS resolution, steamcmd, and reading pages from steamcommunity.com.

`stubby` is used to make DNS requests over HTTPS. `dnsmasq` is used to cache DNS lookups.

`spl-steamcmd.service` is socket activated. It can also shut itself off after not receiving requests for some time to release resources. For some reason, the `docker.io/steamcmd` image, used by `splicer-steamcmd`, is updated frequently. So shutting down those containers periodically gives them a chance to come back with an up-to-date image later on demand. This is made possible by `steamcmd-image-pull.service` and `steamcmd-image-pull.timer` not shown in any diagrams above but it's literally just a oneshot user service that runs `/usr/bin/podman pull docker.io/steamcmd/steamcmd:alpine`.

`spl-api.service` is also socket activated.

One cool thing about the socket activation is that [it works in a container with `--network=none`](https://github.com/containers/podman/blob/v5.7/docs/tutorials/socket_activation.md), where no networking is set up/available in the network namespace. The listen socket is created by the host before the service is running. The socket is passed when the service starts, but the service never needs to actually create the socket to accept connections on it. Since it doesn't need to create the listen socket, it doesn't need addressing or any interfaces to listen on. Ultimately, we still need to create a veth pair and set up some networking ourselves for running steamcmd and ot page steamcommunity.com, but those links are just for outgoing internet traffic and can be firewalled/routed accordingly.


`link-friend.service` runs `link-friend.py` to create a veth pair between the host and the network namespace. It sets the addressing and a default route in the namespace. It doesn't add addressing to the host's end -- instead, the addressing above on spl-peer is added by `systemd-networkd` according to the `etc/systemd/network/99-spl.network` file. (Though using `systemd-networkd` for that is probably more complicated than doing it just in the python script tbh.)

The link on the host is created with group 66 so that you can use nftables to apply rules on that interface using `iifgroup 66`, even before the interface is created and assigned an index.

runtime state, configuration, shared mounts
-------------------------------------------

Some of the services share state on disk at runtime or persist state to disk between runs.

`splicer/api` has a few operational dependencies.

- a config file mounted from the host at `~/api-config/materialist.kdl`
- a sqlite3 database mounted from the host in the directory `~/api-state`
- a socket path to podman, usually at `$XDG_RUNTIME_DIR/podman/podman.sock`
- a podman volume containing the barotrauma vanilla content
- a podman volume containing cloudflare secrets used to publish new content to materialist.pages.dev
- `tar` and `zstd` executables accessible (these are installed from the Containerfile)
- inner and outer paths to temporary directories shared with `splicer-build` containers

`splicer/steamcmd` is configured from command line arguments but does also have other dependencies.

- a socket path to podman, usually at `$XDG_RUNTIME_DIR/podman/podman.sock`
- `tar`, `zstd`, and `podman-remote` executables accessible (these are installed from the Containerfile)
- inner and outer paths to temporary directories shared with `docker.io/steamcmd` containers

The *inner* and *outer* paths are required for the case where a parent container (`splicer/api` or `splicer/steamcmd`) is running another container for the purpose of generating files as output. When creating the child container, the *outer path* is specified as the source to a volume. After the container has written files, the *inner path* is used by the parent container to read the files inside its own namespace.

Both services use podman to start containers. They talk to an external podman over a socket bind mounted into their mount namespace. So the podman they talk to runs outside of these containers. When they create a container, using that outer podman, they specify an `outer path`. That `outer path` will be the path podman uses in its mount namespace. Whereas `inner path` will be the path the container uses in its namespace.

`splicer/api` needs this to read output when running the `splicer-build` image built from `splicer/build/Containerfile`. The paths and image name can be configured under the *build* section of the config file.

`splicer/steamcmd` needs this to read output from the `docker.io/steamcmd` when it downloads workshop files from steam. The paths can be configured as command line arguments.

tldr; a service's *outer path* and *inner path* must lead to the same place, but the *outer path* is the path in the mount namespace for podman and *inner path* is the path in the service's mount namespace. In development, when podman/namespacing is not being used, these paths should be the same.

wow
---

that's it! it's just that shrimple :D


[//]: # (
vim: wrap lbr
)
